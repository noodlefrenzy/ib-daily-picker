---
description: Perform planning and architecture, generating a phase-based plan with success criteria while enforcing clarification and constitution gates before implementation.
---

Please deep think / ultrathink as this is a complex task.

# plan-3-architect

Generate a **comprehensive, phase-based implementation plan** with detailed tasks, TDD structure, and acceptance criteria. This command produces the master plan document that will guide all subsequent implementation phases.

---

## üö´ CRITICAL PROHIBITION: NO TIME ESTIMATES

**NEVER** output time or duration estimates in **ANY FORM**:
- ‚ùå Hours, minutes, days, weeks, months
- ‚ùå "Quick", "fast", "soon", "trivial duration"
- ‚ùå "ETA", "deadline", "timeline"
- ‚ùå "~4 hours", "2-3 days", "should take X time"
- ‚ùå "Total Estimated Effort: X hours"

**ONLY** use **Complexity Score (CS 1-5)** from constitution rubric:
- ‚úÖ CS-1 (trivial): 0-2 complexity points
- ‚úÖ CS-2 (small): 3-4 complexity points
- ‚úÖ CS-3 (medium): 5-7 complexity points
- ‚úÖ CS-4 (large): 8-9 complexity points
- ‚úÖ CS-5 (epic): 10-12 complexity points

**Rubric factors** (each scored 0-2): Scope, Interconnections, Dependencies, Novelty, Fragility, Testing
Reference: `docs/project-rules/constitution.md` ¬ß 9

**Before outputting the plan, validate**: No time language present? All estimates use CS 1-5 only?

---

```md
Inputs:
  FEATURE_SPEC = `docs/plans/<ordinal>-<slug>/<slug>-spec.md` (co-located with plan),
  PLAN_PATH (absolute; MUST match `docs/plans/<ordinal>-<slug>/<slug>-plan.md`),
  rules at `docs/project-rules/{rules.md, idioms.md, architecture.md}`,
  constitution at `docs/project-rules/constitution.md` (if present),
  today {{TODAY}}.

## PHASE 0: Detect Workflow Mode

**FIRST**: Check spec for `**Mode**: Simple` or `**Mode**: Full` in header/metadata.

- If `Mode: Simple` ‚Üí Use **Simple Mode Output Format** (see below)
- If `Mode: Full` or not specified ‚Üí Use **Full Mode Output Format** (standard multi-phase)

**Simple Mode Changes:**
- Single phase instead of multiple phases
- Inline task table (plan-5 format) directly in plan
- Concise findings format (shorter, action-focused)
- Next step prompts show plan-4/plan-5 as optional
- Same subagent depth (4 researchers) for thoroughness

## PHASE 1: Initial Gates & Validation

### GATE - Clarify
- If critical ambiguities remain in SPEC (marked with [NEEDS CLARIFICATION]), instruct running /plan-2-clarify first
- Verify `## Testing Strategy` section exists in spec with defined approach
- User can explicitly override with --skip-clarify flag

### GATE - Constitution
- Validate plan against docs/project-rules/constitution.md if present
- Document any necessary deviations in deviation ledger:

| Principle Violated | Why Needed | Simpler Alternative Rejected | Risk Mitigation |
|-------------------|------------|------------------------------|-----------------|

### GATE - Architecture
- Validate against `docs/project-rules/architecture.md`
- Check for layer-boundary violations (LFL/LSL/Graph/etc.)
- Verify language-agnostic GraphBuilder compliance
- Document any architectural exceptions with justification

### GATE - ADR (Optional)
- Scan `docs/adr/` for ADRs that reference this feature (match by slug/title or spec path)
- Build an ADR Ledger table:

| ADR | Status | Affects Phases | Notes |
|-----|--------|----------------|-------|

- If existing ADRs found, populate table with each ADR's ID (NNNN), status, affected phases, and key constraints
- If a critical design choice is being made in this plan and no ADR exists, recommend running `/plan-3a-adr` before finalizing
- Note: This gate is non-blocking; plan can proceed with or without ADRs

## PHASE 2: Research & Technical Discovery

### Check for Existing Research

**FIRST**: Check if `research-dossier.md` exists in the plan folder:
- If `${PLAN_DIR}/research-dossier.md` exists:
  * Read the research dossier completely
  * Extract all critical and high-impact findings
  * Note discovered patterns, dependencies, and constraints
  * **OPTIMIZE**: Reduce research subagents from 4 to 2 (focus on implementation-specific discovery)
  * Reference research findings throughout the plan (e.g., "Per Research Finding IA-03...")
- If no research exists:
  * Proceed with full 4-subagent research as described below
  * Note: "‚ÑπÔ∏è Consider running `/plan-1a-explore` before planning for deeper understanding"

### Optimized Research Mode (when research-dossier.md exists)

Launch **2 specialized subagents** focusing on implementation planning:

**Subagent 1: Implementation Strategist**
"Based on existing research in research-dossier.md, create implementation strategy.

**Given Research**: [Summary of critical findings from dossier]

**Tasks**:
- Design phase breakdown based on discovered dependencies
- Identify implementation order considering existing patterns
- Plan integration approach using discovered extension points
- Design testing strategy based on existing test infrastructure

**Output**: Implementation-focused discoveries I1-01 through I1-08."

**Subagent 2: Risk & Mitigation Planner**
"Based on research findings, identify implementation risks and mitigation.

**Given Research**: [Danger zones and constraints from dossier]

**Tasks**:
- Map high-risk implementation areas
- Design mitigation strategies for each risk
- Plan rollback approach for dangerous changes
- Identify required safety checks and validations

**Output**: Risk-focused discoveries R1-01 through R1-08."

### Full Research Mode (when no research exists)

**IMPORTANT**: Use **parallel research subagents** for comprehensive and efficient discovery.

**Strategy**: Launch 4 specialized research subagents (single message with 4 Task tool calls) to maximize discovery breadth and depth. Each subagent focuses on a specific domain, uses `/flowspace-research` for evidence gathering, then analyzes and synthesizes findings.

### FlowSpace Evidence Gathering (Hybrid Approach)

Each subagent follows a 2-phase pattern:
1. **Evidence Phase**: Use `/flowspace-research` to gather grounded codebase evidence
2. **Analysis Phase**: Apply domain-specific analysis framework to categorize and synthesize findings

**Fallback**: If FlowSpace unavailable, subagents use Glob/Grep/Read for evidence gathering.

### Parallel Research Architecture

**Subagent 1: Codebase Pattern Analyst**
"Discover existing patterns, conventions, and integration points.

**PHASE 1 - Evidence Gathering** (use /flowspace-research):
```
/flowspace-research "design patterns factory singleton observer repository" --limit 8
/flowspace-research "naming conventions class file function" --limit 8
/flowspace-research "integration points middleware hooks extension" --limit 8
```
If FlowSpace unavailable, use Glob to find similar files and Grep for pattern indicators.

**PHASE 2 - Analysis**:
Based on evidence gathered, analyze and categorize:
- Existing similar features/components and their implementation patterns
- Naming conventions (file naming, class naming, function naming)
- Directory structures and organization
- Design patterns in use (factory, singleton, observer, etc.)
- Integration points where new feature connects to existing systems
- Code conventions (error handling, logging, testing styles)

**Output**: 5-8 discoveries numbered S1-01 through S1-08 covering patterns, integration points, and conventions.

**Format per discovery**:
```markdown
### Discovery S1-01: [Title]
**Category**: Pattern | Integration | Convention
**Impact**: Critical | High | Medium | Low
**Evidence**: [FlowSpace node_id or file:line reference from evidence phase]
**What**: [Concise description]
**Why It Matters**: [How this affects implementation]
**Example**:
‚Äã```[language]
// ‚ùå WRONG - [Why this violates pattern]
[counter-example]

// ‚úÖ CORRECT - [Why this follows pattern]
[good example from codebase]
‚Äã```
**Action Required**: [What implementation must do]
```
"

**Subagent 2: Technical Investigator**
"Identify technical constraints, API limitations, and framework-specific gotchas.

**PHASE 1 - Evidence Gathering** (use /flowspace-research):
```
/flowspace-research "API constraints rate limits quotas version" --limit 8
/flowspace-research "framework gotchas known bugs performance bottleneck" --limit 8
/flowspace-research "security validation input sanitization requirements" --limit 8
```
If FlowSpace unavailable, use Grep to search for error handling, validation patterns, and constraint documentation.

**PHASE 2 - Analysis**:
Based on evidence gathered, identify and root-cause:
- API limitations (rate limits, quotas, restrictions, version compatibility)
- Framework gotchas (known bugs, common mistakes, performance bottlenecks)
- Technical constraints (memory/CPU limits, query limits, file system limitations)
- Security requirements (input validation, sanitization, CORS, CSP)

**Output**: 5-8 discoveries numbered S2-01 through S2-08 covering API limits, framework gotchas, and constraints.

**Format per discovery**:
```markdown
### Discovery S2-01: [Title]
**Category**: API Limit | Framework Gotcha | Constraint
**Impact**: Critical | High | Medium | Low
**Evidence**: [FlowSpace node_id or file:line reference from evidence phase]
**Problem**: [What doesn't work as expected or limitation exists]
**Root Cause**: [Why this limitation exists]
**Solution**: [How to work around it]
**Example**:
‚Äã```[language]
// ‚ùå WRONG - [Why this fails due to limitation]
[bad code example]

// ‚úÖ CORRECT - [Why this works around limitation]
[good code example]
‚Äã```
**References**: [Links to docs, GitHub issues, Stack Overflow]
```
"

**Subagent 3: Discovery Documenter**
"Analyze spec for ambiguities, implications, and edge cases.

**PHASE 1 - Evidence Gathering** (use /flowspace-research):
```
/flowspace-research "error handling edge cases null empty validation" --limit 8
/flowspace-research "backward compatibility migration data schema" --limit 8
/flowspace-research "concurrent access race condition async" --limit 8
```
If FlowSpace unavailable, use Grep to find existing error handling and edge case patterns.

**PHASE 2 - Analysis**:
Based on evidence gathered and spec analysis, identify:
- Spec ambiguities (unclear or underspecified requirements)
- Implementation implications (performance, data migration, backward compatibility, security)
- Edge cases and error scenarios (empty/null input, concurrent access, network failures)

**Output**: 5-8 discoveries numbered S3-01 through S3-08 covering ambiguities, implications, and edge cases.

**Format per discovery**:
```markdown
### Discovery S3-01: [Title]
**Category**: Ambiguity | Implication | Edge Case
**Impact**: Critical | High | Medium | Low
**Evidence**: [FlowSpace node_id or file:line showing existing handling]
**Spec Reference**: [Quote relevant spec section]
**Issue**: [What is unclear, implication, or edge case]
**Design Decision Required**: [What choice must be made]
**Recommendation**: [Suggested approach with rationale]
**Example**: [Scenario with code showing safe/unsafe approaches]
```
"

**Subagent 4: Dependency Mapper**
"Map module dependencies, architectural boundaries, and cross-cutting concerns.

**PHASE 1 - Evidence Gathering** (use /flowspace-research):
```
/flowspace-research "import dependency module requires" --limit 8
/flowspace-research "layer boundary domain service repository" --limit 8
/flowspace-research "logging auth caching metrics configuration cross-cutting" --limit 8
```
If FlowSpace unavailable, use Grep to find import statements and architectural patterns.

**PHASE 2 - Analysis**:
Based on evidence gathered, map and document:
- Module dependencies (what feature depends on, what depends on it)
- Architectural boundaries (layers, domains, cross-boundary communication)
- Cross-cutting concerns (logging, error handling, auth, caching, metrics, config)

**Output**: 5-8 discoveries numbered S4-01 through S4-08 covering dependencies, boundaries, and cross-cutting concerns.

**Format per discovery**:
```markdown
### Discovery S4-01: [Title]
**Category**: Dependency | Boundary | Cross-Cutting Concern
**Impact**: Critical | High | Medium | Low
**Evidence**: [FlowSpace node_id or file:line showing dependency/boundary]
**What**: [Describe dependency/boundary/concern]
**Architectural Context**: [How this fits into system architecture]
**Design Constraint**: [What this means for implementation]
**Example**:
‚Äã```[language]
// ‚ùå VIOLATES BOUNDARY - [Why this breaks architectural rules]
[bad code example]

// ‚úÖ RESPECTS BOUNDARY - [Why this follows architectural rules]
[good code example]
‚Äã```
**Reference**: [Link to docs/project-rules/architecture.md, constitution.md, dependency docs]
```
"

**Wait for All Researchers**: Block until all 4 subagents complete.

### Synthesis Phase

After all 4 subagents complete:
1. **Collect All Discoveries**: Gather S1-01 through S4-08 (approximately 24-32 discoveries)
2. **Deduplicate**: Merge overlapping findings (note sources: S1-03 + S2-05)
3. **Renumber Sequentially**: Assign final discovery numbers 01, 02, 03, ..., NN
   - Order by impact: Critical first, then High, then Medium, then Low
   - Within each tier, order by implementation phase relevance
4. **Format Final Discoveries**:

```markdown
### üö® Critical Discovery 01: [Title]
**Impact**: Critical
**Sources**: [S1-03, S2-05] (pattern analyst + technical investigator)
**Problem**: [What doesn't work as expected]
**Root Cause**: [Why this happens]
**Solution**: [How to work around it or design for it]
**Example**:
‚Äã```[language]
// ‚ùå WRONG - [Why this fails]
[bad code example]

// ‚úÖ CORRECT - [Why this works]
[good code example]
‚Äã```
**Action Required**: [What implementation must do]
**Affects Phases**: [List phase numbers, e.g., Phase 3, Phase 5]
```

### Research Output Requirements
- Minimum 15-20 final discoveries (after deduplication)
- At least 3-5 Critical discoveries
- At least 5-8 High impact discoveries
- All discoveries include code examples
- All discoveries specify affected phases
- Deduplication log showing which subagent findings were merged
- Each final discovery references source subagent discoveries (e.g., "Sources: [S1-03, S4-02]")
- If spec ambiguity discovered, note whether /plan-2-clarify should be re-run

## PHASE 3: Project Structure & Setup

### Project Type Selection
- Determine project type: (single | web | mobile | library | cli | service)
- Generate **actual** directory tree showing all relevant paths
- Use absolute repo-root paths throughout the plan

### Directory Structure Template
```
/path/to/repo/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ [component directories]
‚îÇ   ‚îî‚îÄ‚îÄ [feature modules]
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ plans/
‚îÇ       ‚îî‚îÄ‚îÄ <ordinal>-<slug>/
‚îÇ           ‚îú‚îÄ‚îÄ <slug>-plan.md (this file)
‚îÇ           ‚îî‚îÄ‚îÄ tasks/
‚îÇ               ‚îî‚îÄ‚îÄ [phase directories will be created by plan-5]
‚îî‚îÄ‚îÄ [configuration files]
```

## PHASE 4: Plan Document Generation

### Testing Strategy Adaptation
Read the `## Testing Strategy` section from the spec and adapt plan generation accordingly. Capture both the testing approach and the mock usage preference; reflect both throughout the plan.

**Approach-Specific Guidance:**
- **Full TDD**: Generate comprehensive test-first tasks for all phases (current template)
- **TAD (Test-Assisted Development)**: Generate tasks with Scratch‚ÜíPromote workflow:
  * Create scratch test exploration tasks (tests/scratch/ directory)
  * Implementation tasks interleaved with test refinement
  * Test promotion tasks with Test Doc comment block requirements
  * Every promoted test must include Test Doc block (Why/Contract/Usage Notes/Quality Contribution/Worked Example)
  * Focus on tests that "pay rent" via comprehension value
  * Name tests "Given...When...Then..." format
  * Keep tests/scratch/ out of CI; promoted tests should be reliable and deterministic
- **Lightweight**: Reduce test tasks to core validation only
- **Manual Only**: Replace test tasks with manual verification checklists
- **Hybrid**: Mark phases with approach annotations (TDD/TAD/Lightweight per phase)

### Documentation Strategy Adaptation
Read the `## Documentation Strategy` section from the spec and generate appropriate documentation phases based on the location choice:

- **README.md only**: Create single documentation phase updating root README.md with getting-started content
- **docs/how/ only**: Create documentation phase(s) for detailed guides under docs/how/
  - Structure: `docs/how/<feature-name>/N-topic.md` (numbered files within feature directory)
  - **Intelligent file placement**:
    1. Survey existing `docs/how/` directories to identify relevant feature areas
    2. Decide: create new `docs/how/<new-feature>/` OR use existing `docs/how/<existing-feature>/`
    3. Determine file strategy: create new numbered file OR append to existing file if content is small/related
    4. Use sequential numbering (1-overview.md, 2-usage.md, 3-api.md, etc.)
- **Hybrid**: Create multiple documentation phases:
  - Phase for README.md updates (getting-started, overview, essential commands)
  - Phase for docs/how/ content following structure above
  - Ensure content split matches spec guidance
- **None**: Skip documentation phases (note in plan why docs are not needed)

Documentation phases should include:
- **Discovery step**: List existing docs/how/ feature directories and their content
- **Placement decision**: Document whether creating new feature dir or using existing, with rationale
- **File strategy**: Specify whether creating new files or updating existing ones
- Clear file paths (absolute: /path/to/repo/README.md or /path/to/repo/docs/how/<feature>/N-topic.md)
- Content outlines (what sections/topics to cover)
- Target audience considerations
- Maintenance/update expectations
- **ADR references**: Link relevant ADR(s) in the plan's References & per-phase Acceptance Criteria if they constrain the work

- **Full TDD**: Generate comprehensive test-first tasks for all phases (current template)
- **Lightweight**:
  - Reduce test tasks to core validation only
  - Skip unit test tasks for simple operations
  - Focus on integration/smoke tests
- **Manual Only**:
  - Replace test tasks with "Document manual validation steps"
  - Include manual test checklist in acceptance criteria
- **Hybrid**:
  - Mark complex phases for full TDD
  - Mark simple phases for lightweight testing
  - Clearly indicate testing approach per phase

### Required Sections (in order)

#### 1. Title Block & Metadata
```markdown
# [Feature Name] Implementation Plan

**Plan Version**: 1.0.0
**Created**: {{TODAY}}
**Spec**: [link to ./<slug>-spec.md]
**Status**: DRAFT | READY | IN_PROGRESS | COMPLETE
```

#### 2. Table of Contents (MANDATORY)
- Must include all major sections
- Link to each phase
- Include appendices

#### 3. Executive Summary
- Problem statement (2-3 sentences)
- Solution approach (bullet points)
- Expected outcomes
- Success metrics

#### 4. Technical Context
- Current system state
- Integration requirements
- Constraints and limitations
- Assumptions

#### 5. Critical Research Findings
- Include all discoveries from Phase 2
- Order by impact (highest first)
- Cross-reference to affected phases

#### 6. Testing Philosophy
```markdown
### Testing Approach
[Reference the Testing Strategy from spec]
- **Selected Approach**: [Full TDD | TAD | Lightweight | Manual | Hybrid]
- **Rationale**: [From spec]
- **Focus Areas**: [From spec]

### Test-Driven Development (if applicable)
[Include if Full TDD or Hybrid selected]
- Write tests FIRST (RED)
- Implement minimal code (GREEN)
- Refactor for quality (REFACTOR)

### Test-Assisted Development (TAD) (if applicable)
[Include if TAD selected]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
#### ‚ö†Ô∏è TEST EXECUTION REQUIREMENT (MANDATORY FOR TAD)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

**TAD is not possible without executing tests repeatedly.**

Implementers MUST:
- **RUN** scratch tests after writing them (RED phase)
- **RUN** tests after each code change (GREEN phase)
- **RUN** tests after refactoring (verification)
- Provide test execution output as evidence
- Demonstrate 10-20+ RED‚ÜíGREEN cycles per feature

```bash
# Example Python test execution
pytest tests/scratch/test_feature.py -v --tb=short

# Example TypeScript test execution
npm test tests/scratch/test-feature.test.ts
```

**Success criteria must include**: "Test runner output shows X RED‚ÜíGREEN cycles"

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

- Tests are executable documentation optimized for developer comprehension
- **Scratch ‚Üí RUN ‚Üí Promote workflow**:
  1. Write probe tests in tests/scratch/ to explore/iterate (fast, excluded from CI)
  2. **üî¥üü¢ RUN scratch tests repeatedly** in tight feedback loop (RED‚ÜíGREEN cycle):
     * Write scratch test for small isolated behavior
     * **RUN test** (expect failure - RED) ‚Üê **EXECUTE WITH TEST RUNNER**
     * Write minimal code to pass test
     * **RUN test again** (expect success - GREEN) ‚Üê **EXECUTE WITH TEST RUNNER**
     * Refactor if needed, **re-run test**
     * REPEAT for next behavior
     * This high-fidelity loop validates isolated code WITHOUT running entire project
  3. Implement code iteratively, refining behavior after each test run
  4. When behavior stabilizes, promote valuable tests (typically 1-2 per feature, ~5-10% promotion rate) to tests/unit/ or tests/integration/
  5. Add Test Doc comment contract to each promoted test (required fields below)
  6. Delete scratch probes that don't add durable value (expect to delete 90-95%); keep learning notes in PR
- **Promotion heuristic (apply ruthlessly)**: Keep if Critical path, Opaque behavior, Regression-prone, or Edge case
- **Test naming format**: "Given...When...Then..." (e.g., `test_given_iso_date_when_parsing_then_returns_normalized_cents`)
- **Test Doc comment block** (required for every promoted test):
  ```
  /*
  Test Doc:
  - Why: <business/bug/regression reason in 1‚Äì2 lines>
  - Contract: <plain-English invariant(s) this test asserts>
  - Usage Notes: <how a developer should call/configure the API; gotchas>
  - Quality Contribution: <what failure this will catch; link to issue/PR/spec>
  - Worked Example: <inputs/outputs summarized for scanning>
  */
  ```
- **Quality principles**: Tests must explain why they exist, what contract they lock in, and how to use the code
- **CI requirements**: Exclude tests/scratch/ from CI; promoted tests must be deterministic without network/sleep/flakes (performance requirements specified in spec when needed)

### Lightweight Testing (if applicable)
[Include if Lightweight or Hybrid selected]
- Focus on core functionality validation
- Skip extensive unit testing for simple operations
- Prioritize integration and smoke tests

### Test Documentation (when tests are written)
Every test must include:
"""
Purpose: [what truth this test proves]
Quality Contribution: [how this prevents bugs]
Acceptance Criteria: [measurable assertions]
"""

### Mock Usage (align with spec)
- If spec says "Avoid mocks": use real data/fixtures; only stub truly external calls (network/SaaS)
- If spec says "Targeted mocks": permit mocks for explicitly slow/external dependencies; document rationale per phase
- If spec says "Liberal mocks": allow mocks/stubs wherever they improve clarity or speed; ensure acceptance criteria still cover end-to-end behavior
```

#### 7. Implementation Phases

For EACH phase, generate:

### Phase N: [Descriptive Title]

**Objective**: [Single sentence goal]

**Deliverables**:
- [Concrete deliverable 1]
- [Concrete deliverable 2]

**Dependencies**:
- Phase X must be complete (if applicable)
- External systems available
- Test data prepared

**Risks**:
| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| [Risk description] | Low/Med/High | Low/Med/High | [Mitigation strategy] |

### Tasks (Adapt based on Testing Strategy)

For **Full TDD** approach:
| #   | Status | Task | CS | Success Criteria | Log | Notes |
|-----|--------|------|----|------------------|-----|-------|
| N.1 | [ ] | Write comprehensive tests for [component] | 2 | Tests cover: [scenarios], all fail initially | - | |
| N.2 | [ ] | Implement [component] to pass tests | 3 | All tests from N.1 pass | - | |
| N.3 | [ ] | Write integration tests for [feature] | 2 | Tests document expected behavior | - | |
| N.4 | [ ] | Integrate [component] with [system] | 3 | Integration tests pass | - | |
| N.5 | [ ] | Refactor for [quality aspect] | 2 | Code meets idioms, tests still pass | - | |

For **TAD (Test-Assisted Development)** approach:
| #   | Status | Task | CS | Success Criteria | Log | Notes |
|-----|--------|------|----|------------------|-----|-------|
| N.1 | [ ] | Create tests/scratch/ directory | 1 | Directory exists, excluded from CI config | - | Ensure .gitignore or CI config excludes tests/scratch/ |
| N.2 | [ ] | Write scratch probes for [component] | 2 | 10-15 probe tests exploring behavior | - | Fast iteration, no Test Doc blocks needed |
| N.3 | [ ] | **RUN-Implement-Fix loop** for [component] | 3 | **Test runner output shows 10-20 RED‚ÜíGREEN cycles** | - | **MUST EXECUTE**: pytest/npm test commands run repeatedly, paste output as evidence |
| N.4 | [ ] | Complete implementation | 3 | Core functionality works, all scratch tests pass | - | Code refined through iterative testing |
| N.5 | [ ] | Promote valuable tests to tests/unit/ | 2 | 1-2 tests moved (~5-10% of scratch tests) with Test Doc blocks added | - | Apply heuristic ruthlessly: Critical path, Opaque behavior, Regression-prone, Edge case |
| N.6 | [ ] | Add Test Doc comment blocks | 2 | All promoted tests have Why/Contract/Usage/Quality/Example | - | Required 5 fields per promoted test |
| N.7 | [ ] | Delete non-valuable scratch tests | 1 | 90-95% of scratch tests deleted, only promoted tests remain in main suite | - | Keep learning notes in execution log/PR |
| N.8 | [ ] | Verify CI exclusion of scratch/ | 1 | CI config explicitly excludes tests/scratch/ | - | |

For **Lightweight** approach:
| #   | Status | Task | CS | Success Criteria | Log | Notes |
|-----|--------|------|----|------------------|-----|-------|
| N.1 | [ ] | Implement [component] | 2 | Basic functionality works | - | |
| N.2 | [ ] | Write validation test | 1 | Core behavior verified | - | |
| N.3 | [ ] | Run smoke test | 1 | End-to-end flow works | - | |

For **Manual Only** approach:
| #   | Status | Task | CS | Success Criteria | Log | Notes |
|-----|--------|------|----|------------------|-----|-------|
| N.1 | [ ] | Implement [component] | 2 | Functionality complete | - | |
| N.2 | [ ] | Document manual test steps | 1 | Clear verification process | - | |
| N.3 | [ ] | Execute manual validation | 1 | All checks pass | - | |

For **Hybrid** approach:
[Mark each phase as Full TDD, TAD, or Lightweight based on complexity and documentation needs]

### Test Examples (Write First!)

```[language]
describe('[Component]', () => {
    test('should [specific behavior]', () => {
        """
        Purpose: Proves [component] correctly handles [scenario]
        Quality Contribution: Prevents [type of bug]
        Acceptance Criteria:
        - [Assertion 1]
        - [Assertion 2]
        """

        // Arrange
        const input = [test data];

        // Act
        const result = component.method(input);

        // Assert
        expect(result.property).toBe(expectedValue);
        expect(result.state).toMatch(pattern);
    });

    test('should handle [edge case]', () => {
        """
        Purpose: Ensures system remains stable when [condition]
        Quality Contribution: Prevents crashes in production
        Acceptance Criteria: Graceful error handling
        """

        // Test implementation
    });
});
```

### Non-Happy-Path Coverage
- [ ] Null/undefined inputs handled
- [ ] Concurrent access scenarios tested
- [ ] Error propagation verified
- [ ] Resource cleanup confirmed

### Acceptance Criteria
- [ ] All tests passing (100% of phase tests)
- [ ] Test coverage > 80% for new code
- [ ] Mock usage conforms to spec preference (document deviations)
- [ ] Documentation updated
- [ ] ADR constraints respected (list ADR-NNNN IDs where applicable)

#### 8. Cross-Cutting Concerns

### Security Considerations
- Input validation strategy
- Authentication/authorization requirements
- Sensitive data handling

### Observability
- Logging strategy
- Metrics to capture
- Error tracking approach

### Documentation
- Documentation location (per Documentation Strategy from spec)
- Content structure and organization
- Update/maintenance schedule
- Target audience and accessibility

#### 9. Complexity Tracking

Track component-level complexity using CS 1-5 scale:

| Component | CS | Label | Breakdown (S,I,D,N,F,T) | Justification | Mitigation |
|-----------|-----|-------|------------------------|---------------|------------|
| [Component] | 4 | Large | S=2,I=1,D=2,N=1,F=1,T=2 | [Why this complexity unavoidable] | [Rollout plan, flags, monitoring] |

**When to populate**:
- Any component with CS ‚â• 3 (medium or higher)
- Constitution/Architecture deviations
- Technical debt accumulation points
- High-risk integration points

**CS mapping**: CS-1 (trivial), CS-2 (small), CS-3 (medium), CS-4 (large), CS-5 (epic)

**For CS ‚â• 4**: Mitigation MUST include staged rollout, feature flags, and rollback plan

#### 10. Progress Tracking

### Phase Completion Checklist
- [ ] Phase 1: [Title] - [Status]
- [ ] Phase 2: [Title] - [Status]
- [ ] Phase 3: [Title] - [Status]
- [ ] Phase 4: [Title] - [Status]
- [ ] Phase 5: [Title] - [Status]

### STOP Rule
**IMPORTANT**: This plan must be complete before creating tasks. After writing this plan:
1. Run `/plan-4-complete-the-plan` to validate readiness
2. Only proceed to `/plan-5-phase-tasks-and-brief` after validation passes

#### 11. Change Footnotes Ledger

**NOTE**: This section will be populated during implementation by plan-6a-update-progress.

**Footnote Numbering Authority**: plan-6a-update-progress is the **single source of truth** for footnote numbering across the entire plan.

**Allocation Strategy**:
- plan-6a reads the current ledger and determines the next available footnote number
- Footnote numbers are sequential and shared across all phases and subtasks (e.g., [^1], [^2], [^3]...)
- Each invocation of plan-6a increments the counter and updates BOTH ledgers (plan and dossier) atomically
- Footnotes are never manually assigned; always delegated to plan-6a for consistency

**Format**:
```markdown
[^N]: Task {plan-task-id} - {one-line summary}
  - `{flowspace-node-id}`
  - `{flowspace-node-id}`
```

**Example Template**:
```markdown
## Change Footnotes Ledger

[^1]: Task 2.3 - Added validation function
  - `function:src/validators/input_validator.py:validate_user_input`
  - `function:src/validators/input_validator.py:sanitize_input`

[^2]: Task 2.3 - Updated authentication flow
  - `method:src/auth/service.py:AuthService.authenticate`
  - `method:src/auth/service.py:AuthService.validate_token`

[^3]: Task 2.4 - Configuration changes
  - `file:config/settings.py`
  - `file:config/validators.json`
```

**Initial State** (before implementation begins):
```markdown
## Change Footnotes Ledger

[^1]: [To be added during implementation via plan-6a]
[^2]: [To be added during implementation via plan-6a]
...
```

## PHASE 5: Validation & Output

### Pre-Write Validation Checklist
- [ ] TOC includes all sections
- [ ] All phases have numbered tasks
- [ ] Each task has clear success criteria
- [ ] Test examples provided for each phase
- [ ] TDD approach evident (tests before implementation)
- [ ] Mock usage policy mirrors spec
- [ ] Absolute paths used throughout
- [ ] Dependencies clearly stated
- [ ] Risks identified with mitigations
- [ ] Acceptance criteria measurable
- [ ] Cross-cutting concerns addressed
- [ ] Constitution/Architecture gates passed

### Output Requirements
1. Create parent directory if needed: `docs/plans/<ordinal>-<slug>/`
2. Write plan to: `docs/plans/<ordinal>-<slug>/<slug>-plan.md`
3. Ensure plan is self-contained (no assumed context)
4. Include all code examples inline
5. Use mermaid diagrams where helpful

### Success Message Template
```
‚úÖ Plan created successfully:
- Location: [absolute path to plan]
- Phases: [count]
- Total tasks: [count]
- Next step: Run /plan-4-complete-the-plan to validate
```

## Example Phase (For Reference)

Note: This example shows Full TDD approach. Adapt based on Testing Strategy from spec.

### Phase 1: Core BridgeContext Infrastructure

**Objective**: Create the foundational BridgeContext interface and basic service structure using TDD.

**Deliverables**:
- BridgeContext interface with VS Code API wrappers
- Factory pattern implementation
- Logger service using OutputChannel
- Comprehensive test coverage

**Dependencies**: None (foundational phase)

**Risks**:
| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| VS Code API changes | Low | High | Pin VS Code engine version |
| Test isolation issues | Medium | Medium | Use test fixtures |

### Tasks (TDD Approach)

| #   | Status | Task | CS | Success Criteria | Log | Notes |
|-----|--------|------|----|------------------|-----|-------|
| 1.1 | [ ] | Write comprehensive tests for BridgeContext | 2 | Tests cover: version check, getWorkspace, getConfiguration, error cases | - | Create BridgeContext.test.ts |
| 1.2 | [ ] | Write tests for BridgeContext factory | 2 | Tests cover: singleton behavior, lifecycle, context injection | - | Create factory.test.ts |
| 1.3 | [ ] | Create BridgeContext TypeScript interface | 1 | Interface compiles, exports properly | - | Define in types.ts |
| 1.4 | [ ] | Implement BridgeContext to pass tests | 3 | All tests from 1.1 pass | - | Thin wrappers around VS Code APIs |
| 1.5 | [ ] | Implement factory to pass tests | 2 | All tests from 1.2 pass | - | Singleton pattern |
| 1.6 | [ ] | Write tests for logger service | 2 | Tests cover: log levels, output channel, formatting | - | Integrate with BridgeContext.test.ts |
| 1.7 | [ ] | Implement logger service | 2 | Logger uses OutputChannel, all tests pass | - | Use VS Code OutputChannel API |
| 1.8 | [ ] | Create index exports and validate | 1 | Can import from 'core/bridge-context' | - | Clean module exports |

### Test Examples (Write First!)

```typescript
import * as assert from 'assert';
import * as vscode from 'vscode';
import { BridgeContext } from '../../../extension/src/core/bridge-context';

suite('BridgeContext using VS Code APIs', () => {
    let context: BridgeContext;

    setup(async () => {
        """
        Purpose: Ensure clean test state for each test
        Quality Contribution: Prevents test interdependencies
        Acceptance Criteria: Fresh context for each test
        """
        const ext = vscode.extensions.getExtension('your.extension.id')!;
        await ext.activate();
        context = new BridgeContext(ext.exports.getContext());
    });

    test('should return current version', () => {
        """
        Purpose: Proves version property is accessible and correct
        Quality Contribution: Enables version-specific behavior
        Acceptance Criteria:
        - Returns string version
        - Matches package.json version
        - Property is readonly
        """

        assert.strictEqual(typeof context.version, 'string');
        assert.strictEqual(context.version, '1.0.0');
        assert.throws(() => {
            (context as any).version = '2.0.0';
        });
    });

    test('should handle missing workspace gracefully', () => {
        """
        Purpose: Ensures stability when no workspace is open
        Quality Contribution: Prevents crashes in extension activation
        Acceptance Criteria: Returns undefined, no exceptions
        """

        const workspace = context.getWorkspace();
        if (!vscode.workspace.workspaceFolders) {
            assert.strictEqual(workspace, undefined);
        }
    });
});
```

### Non-Happy-Path Coverage
- [ ] Null context handling
- [ ] Disposed extension context
- [ ] Missing VS Code APIs (older versions)
- [ ] Concurrent initialization attempts

### Acceptance Criteria
- [ ] All tests passing (18 tests)
- [ ] No mocks used (real VS Code APIs)
- [ ] Test coverage > 90%
- [ ] Clean module exports
- [ ] TypeScript strict mode passes

## Example Documentation Phase (For Reference)

Note: This example shows a Hybrid documentation approach (README + docs/how/). Adapt based on Documentation Strategy from spec.

### Phase N: Documentation

**Objective**: Document the BridgeContext feature for users and maintainers following hybrid approach (essentials in README, details in docs/how/bridge-context/).

**Deliverables**:
- Updated README.md with getting-started guide
- Detailed guides in docs/how/bridge-context/ (numbered structure)

**Dependencies**: All implementation phases complete, tests passing

**Risks**:
| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Documentation drift | Medium | Medium | Include doc updates in phase acceptance criteria |
| Unclear examples | Low | Medium | Use real code snippets from implementation |

### Discovery & Placement Decision

**Existing docs/how/ structure**:
```
docs/how/
‚îú‚îÄ‚îÄ testing/
‚îÇ   ‚îú‚îÄ‚îÄ 1-overview.md
‚îÇ   ‚îî‚îÄ‚îÄ 2-tdd-workflow.md
‚îî‚îÄ‚îÄ architecture/
    ‚îî‚îÄ‚îÄ 1-overview.md
```

**Decision**: Create new `docs/how/bridge-context/` directory (no existing relevant feature area)

**File strategy**: Create new numbered files (1-overview.md, 2-usage.md, 3-api.md)

### Tasks (Lightweight Approach for Documentation)

| #   | Status | Task | CS | Success Criteria | Log | Notes |
|-----|--------|------|----|------------------|-----|-------|
| N.1 | [ ] | Survey existing docs/how/ directories | 1 | Documented existing structure, identified no conflicts | - | Discovery step |
| N.2 | [ ] | Update README.md with BridgeContext getting-started | 2 | Installation, basic usage, link to docs/how/bridge-context/ | - | /path/to/repo/README.md |
| N.3 | [ ] | Create docs/how/bridge-context/1-overview.md | 2 | Introduction, motivation, architecture diagram complete | - | /path/to/repo/docs/how/bridge-context/1-overview.md |
| N.4 | [ ] | Create docs/how/bridge-context/2-usage.md | 2 | Step-by-step usage guide with code examples | - | /path/to/repo/docs/how/bridge-context/2-usage.md |
| N.5 | [ ] | Create docs/how/bridge-context/3-api.md | 2 | All public APIs documented with examples | - | /path/to/repo/docs/how/bridge-context/3-api.md |
| N.6 | [ ] | Review documentation for clarity and completeness | 1 | Peer review passed, no broken links | - | All docs reviewed |

### Content Outlines

**README.md section** (Hybrid: getting-started only):
- What is BridgeContext (1-2 sentences)
- Installation/setup (essential steps)
- Basic usage example (minimal code snippet)
- Link to detailed docs: `docs/how/bridge-context/`

**docs/how/bridge-context/1-overview.md**:
- Introduction and motivation
- Architecture diagram
- Key concepts
- When to use BridgeContext

**docs/how/bridge-context/2-usage.md**:
- Installation and configuration
- Common use cases with examples
- Code snippets (tested)
- Troubleshooting section

**docs/how/bridge-context/3-api.md**:
- API reference for all public interfaces
- Parameter descriptions and types
- Return types
- Code examples for each method

### Acceptance Criteria
- [ ] README.md updated with getting-started section
- [ ] All docs/how/bridge-context/ files created and complete
- [ ] Code examples tested and working
- [ ] No broken links (internal or external)
- [ ] Peer review completed
- [ ] Target audience can follow guides successfully
- [ ] Numbered file structure follows convention

## Simple Mode Output Format

**When spec has `Mode: Simple`**, generate a streamlined single-phase plan:

### Simple Mode Plan Structure

```markdown
# [Feature Name] Implementation Plan

**Mode**: Simple
**Plan Version**: 1.0.0
**Created**: {{TODAY}}
**Spec**: [link to ./<slug>-spec.md]
**Status**: DRAFT | READY | IN_PROGRESS | COMPLETE

## Table of Contents
1. [Executive Summary](#executive-summary)
2. [Critical Research Findings](#critical-research-findings)
3. [Implementation](#implementation)
4. [Change Footnotes Ledger](#change-footnotes-ledger)

## Executive Summary
[2-3 sentences: Problem, Solution approach, Expected outcome]

## Critical Research Findings (Concise)

**Format for Simple Mode** - Action-focused, one finding per line:

| # | Impact | Finding | Action |
|---|--------|---------|--------|
| 01 | Critical | [Title: One-line description] | [What implementation must do] |
| 02 | High | [Title: One-line description] | [What implementation must do] |
| 03 | High | [Title: One-line description] | [What implementation must do] |
| ... | ... | ... | ... |

**Note**: Still generate 15-20+ findings via 4 subagents, but present in concise table format.

## Implementation (Single Phase)

**Objective**: [One sentence goal]

**Testing Approach**: [From spec - Lightweight/Manual/TDD/TAD/Hybrid]
**Mock Usage**: [From spec - Avoid/Targeted/Liberal]

### Tasks

| Status | ID | Task | CS | Type | Dependencies | Absolute Path(s) | Validation | Notes |
|--------|-----|------|----|------|--------------|------------------|------------|-------|
| [ ] | T001 | [Task description] | 2 | Setup | -- | /abs/path/to/file | [Success criteria] | |
| [ ] | T002 | [Task description] | 2 | Core | T001 | /abs/path/to/file | [Success criteria] | |
| [ ] | T003 | [Task description] | 2 | Core | T001 | /abs/path/to/file | [Success criteria] | |
| [ ] | T004 | [Task description] | 1 | Test | T002,T003 | /abs/path/to/test | [Success criteria] | |

**Task Table Notes**:
- Uses same 9-column format as plan-5 dossiers
- Tasks are detailed enough for direct implementation (no plan-5 expansion needed)
- Include absolute paths for all files
- CS scores follow constitution rubric

### Acceptance Criteria
- [ ] [Criterion 1 - testable/observable]
- [ ] [Criterion 2 - testable/observable]
- [ ] [Criterion 3 - testable/observable]

### Risks
| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| [Risk] | Low/Med/High | Low/Med/High | [Mitigation] |

## Change Footnotes Ledger

[^1]: [To be added during implementation via plan-6a]
...

---

**Next steps:**
- **Ready to implement**: `/plan-6-implement-phase --plan "<path>"`
- **Optional validation**: `/plan-4-complete-the-plan` (recommended for CS-3+ tasks)
- **Optional task expansion**: `/plan-5-phase-tasks-and-brief` (if you want a separate dossier)
```

### Simple Mode Success Message

```
‚úÖ Plan created successfully (Simple Mode):
- Location: [absolute path to plan]
- Mode: Simple (single phase, inline tasks)
- Tasks: [count]
- Next step: Run /plan-6-implement-phase --plan "<path>"
- Optional: Run /plan-4-complete-the-plan for validation
```

### Key Differences from Full Mode

| Aspect | Full Mode | Simple Mode |
|--------|-----------|-------------|
| Phases | Multiple (3-5 typical) | Single |
| Task table | High-level (expanded by plan-5) | Detailed inline (plan-5 format) |
| Findings format | Detailed with code examples | Concise table format |
| plan-4 | Required | Optional |
| plan-5 | Required | Optional |
| Next step | /plan-4-complete-the-plan | /plan-6-implement-phase |

---

## Style & Formatting Rules

- Use Markdown headings hierarchically (# > ## > ### > ####)
- Keep one blank line between sections
- Wrap lines at ~100 chars for readability
- Use tables for structured data
- Include language hints in code blocks
- Number all phases and tasks consistently
- Use checkboxes for status tracking
- Provide absolute paths (no relative paths)

---

## Appendix A: Anchor Naming Conventions

All deep links in the FlowSpace provenance graph use kebab-case anchors for consistency and reliability.

### Phase Anchors
**Format**: `phase-{number}-{slug}`
**Example**: `phase-2-input-validation`

Generated from: "Phase 2: Input Validation"

### Task Anchors (Plan)
**Format**: `task-{number}-{slug}` (use plan task number like "23" for task 2.3)
**Example**: `task-23-implement-validation`

Generated from: Task 2.3 with name "Implement validation"
Note: Use the flattened number (2.3 ‚Üí 23) for uniqueness

### Task Anchors (Dossier)
**Format**: `task-{id}-{slug}` (use T-ID like "t003")
**Example**: `task-t003-implement-validation`

Generated from: Dossier task T003 with name "Implement validation"
Note: Includes "t" prefix to distinguish from plan task anchors

### Table Anchors
**Format**: `tasks-{approach}-approach` (based on testing approach)
**Examples**:
- `tasks-full-tdd-approach`
- `tasks-tad-approach`
- `tasks-lightweight-approach`
- `tasks-manual-approach`
- `tasks-hybrid-approach`

Generated from: Testing approach specified in plan ¬ß 6 Testing Strategy

### Subtask Anchors
**Format**: `{ordinal}-subtask-{slug}`
**Example**: `003-subtask-bulk-import-fixtures`

Generated from: Subtask ordinal (003) + slugified name

### Slugification Rules

**Algorithm** (used by plan-5, plan-5a, plan-6a):
1. Convert to lowercase
2. Replace spaces with hyphens
3. Replace non-alphanumeric characters (except hyphens) with hyphens
4. Collapse multiple consecutive hyphens to single hyphen
5. Trim leading and trailing hyphens

**Command**:
```bash
ANCHOR=$(echo "${INPUT}" | tr ' ' '-' | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9-]/-/g' | sed 's/--*/-/g' | sed 's/^-//;s/-$//')
```

**Examples**:
- "Phase 2: Input Validation" ‚Üí `phase-2-input-validation`
- "Task 2.3: Implement Validation" ‚Üí `task-23-implement-validation` (plan) or `task-t003-implement-validation` (dossier)
- "Full TDD Approach" ‚Üí `tasks-full-tdd-approach`
- "003-subtask-bulk import_fixtures!" ‚Üí `003-subtask-bulk-import-fixtures`

### Anchor Stability

**IMPORTANT**: Once anchors are created and referenced, they should not change. Modifying task names or phase titles should NOT break existing deep links.

**Best Practice**: If a task name must change after implementation begins:
1. Keep the original anchor unchanged
2. Update only the visible heading text
3. Verify all deep links still resolve correctly

---

## Appendix B: Graph Traversal Guide

The FlowSpace planning system creates a **bidirectional provenance graph** connecting tasks, logs, files, and footnotes. This guide shows how to navigate the graph in all directions.

### Graph Node Types

1. **Plan Tasks** - Tasks in plan.md ¬ß 8 (numbered 2.3, 4.1, etc.)
2. **Dossier Tasks** - Tasks in `tasks/phase-N/tasks.md` (numbered T001, T002, ST001, etc.)
3. **Execution Log Entries** - In `tasks/phase-N/execution.log.md`
4. **Modified Files** - Source code, docs, configs with embedded FlowSpace IDs
5. **Footnotes** - In plan.md ¬ß 11 and dossier ¬ß Phase Footnote Stubs

### Navigation Patterns

#### From Task ‚Üí Everything

**Starting Point**: Dossier task T003 in `tasks/phase-2/tasks.md`

1. **Find execution log entries**:
   - Look in Notes column for: `log#task-23-implement-validation`
   - Open: `tasks/phase-2/execution.log.md#task-23-implement-validation`
   - View implementation notes, test results, timing, and decisions

2. **Find modified files**:
   - Look in Absolute Path(s) column: `/abs/path/to/validators.py, /abs/path/to/auth.py`
   - Look in Notes column for footnote: `[^3]`
   - Jump to footnote ledger (bottom of tasks.md or plan.md ¬ß 11)
   - Read FlowSpace node IDs:
     * `function:src/validators.py:validate_email`
     * `method:src/auth/service.py:AuthService.authenticate`
   - Open files and navigate to specific symbols

3. **Find plan task**:
   - Look in Notes column for: "Supports plan task 2.3"
   - Open: `../../plan.md#task-23-implement-validation`
   - View plan-level task details and acceptance criteria

4. **Find subtasks** (if any):
   - Look in Subtasks column: `001-subtask-fixtures, 003-subtask-bulk`
   - Open: `tasks/phase-2/001-subtask-fixtures.md`
   - View subtask dossier and ST### tasks

#### From File ‚Üí Tasks

**Starting Point**: Source file `src/validators.py`

1. **Find embedded FlowSpace ID comments**:
   ```python
   # FlowSpace: [^3] [^7] [^12] function:src/validators.py:validate_email
   def validate_email(email: str) -> bool:
       ...
   ```
   - Note footnote numbers: `[^3]`, `[^7]`, `[^12]`
   - These represent all tasks that ever modified this function

2. **Look up footnotes in plan**:
   - Open: `plan.md` ¬ß 11 Change Footnotes Ledger
   - Find: `[^3]: Task 2.3 - Added validation function`
   - Find: `[^7]: Task 3.2 - Enhanced email validation`
   - Find: `[^12]: Task 4.1 - Added internationalization support`

3. **Navigate to tasks**:
   - From footnote, note task IDs: "2.3", "3.2", "4.1"
   - Open plan tasks: `plan.md#task-23`, `plan.md#task-32`, `plan.md#task-41`
   - Or navigate to dossier tasks via plan task links

**Result**: Complete modification history showing which tasks touched this file and why

#### From Execution Log ‚Üí Task ‚Üí Files

**Starting Point**: Log entry in `execution.log.md`

1. **Read log metadata**:
   ```markdown
   ## Task 2.3: Implement validation
   **Dossier Task**: T003
   **Plan Task**: 2.3
   **Plan Reference**: [Phase 2: Input Validation](../../plan.md#phase-2-input-validation)
   **Dossier Reference**: [View T003 in Dossier](./tasks.md#task-t003)
   **Plan Task Entry**: [View Task 2.3 in Plan](../../plan.md#task-23-implement-validation)
   ```

2. **Navigate to tasks**:
   - Click dossier link ‚Üí `tasks.md#task-t003`
   - Click plan link ‚Üí `plan.md#task-23-implement-validation`
   - View task details, dependencies, validation criteria

3. **From task, find modified files**:
   - Check Absolute Path(s) column for direct file paths
   - Check Notes column for footnote: `[^3]`
   - Look up `[^3]` in footnote ledger
   - Get FlowSpace node IDs for specific symbols
   - Open files and navigate to symbols

#### From Footnote ‚Üí Everything

**Starting Point**: Footnote `[^3]` in plan.md ¬ß 11

1. **Read footnote content**:
   ```markdown
   [^3]: Task 2.3 - Added validation function
     - `function:src/validators/input_validator.py:validate_user_input`
     - `function:src/validators/input_validator.py:sanitize_input`
     - `function:src/validators/input_validator.py:validate_email_format`
   ```

2. **Navigate to task**:
   - Note task ID: "2.3"
   - Open: `plan.md#task-23-implement-validation`
   - View task acceptance criteria and status

3. **Navigate to files**:
   - Extract file path from FlowSpace IDs: `src/validators/input_validator.py`
   - Open file
   - Search for embedded FlowSpace ID comments with `[^3]`
   - Navigate to specific functions

4. **Navigate to execution log**:
   - From task, find log reference: `log#task-23-implement-validation`
   - Open: `tasks/phase-2/execution.log.md#task-23-implement-validation`
   - View implementation details and test results

#### From Subtask ‚Üí Parent Task

**Starting Point**: Subtask dossier `tasks/phase-2/001-subtask-fixtures.md`

1. **Read Parent Context section**:
   ```markdown
   ## Parent Context
   **Parent Task(s):** [T003: Implement validation](../tasks.md#task-t003)
   **Plan Task(s):** [2.3: Implement validation](../../plan.md#task-23-implement-validation)
   **Why This Subtask:** Test fixtures needed before implementing validation...
   ```

2. **Navigate to parent**:
   - Click parent dossier link ‚Üí `tasks.md#task-t003`
   - Click plan task link ‚Üí `plan.md#task-23-implement-validation`
   - View parent task context

3. **Check resumption status**:
   - In plan.md, find Subtasks Registry
   - Locate row for `001-subtask-fixtures`
   - Check Status: `[x] Complete` or `[ ] Pending`

#### From Parent Task ‚Üí Subtasks

**Starting Point**: Dossier task T003 in `tasks/phase-2/tasks.md`

1. **Check Subtasks column**:
   - Look for: `001-subtask-fixtures, 003-subtask-bulk`
   - Note: comma-separated list of subtask IDs

2. **Navigate to subtask dossiers**:
   - Open: `tasks/phase-2/001-subtask-fixtures.md`
   - Open: `tasks/phase-2/003-subtask-bulk.md`
   - View subtask tasks (ST### format) and alignment brief

3. **Check subtask status in registry**:
   - Open: `plan.md` ¬ß Subtasks Registry
   - Find rows for subtasks
   - Check completion status

### Common Traversal Scenarios

**Scenario 1: "Which tasks modified this file?"**
- Open file ‚Üí Find FlowSpace ID comments ‚Üí Extract footnote numbers ‚Üí Look up in plan ¬ß 11 ‚Üí Get task IDs

**Scenario 2: "What did task 2.3 actually change?"**
- Open plan.md#task-23 ‚Üí Check footnote in Notes ‚Üí Look up in ¬ß 11 ‚Üí Get FlowSpace node IDs ‚Üí Open files

**Scenario 3: "Why was this function added?"**
- Open file ‚Üí Find FlowSpace ID with footnote ‚Üí Look up footnote ‚Üí Get task ID ‚Üí Open log entry ‚Üí Read "Why" and rationale

**Scenario 4: "What's the history of this class?"**
- Open file ‚Üí Find FlowSpace ID with multiple footnotes (e.g., `[^3] [^7] [^12]`) ‚Üí Look up each footnote ‚Üí Get chronological task list

**Scenario 5: "Is this subtask blocking the parent task?"**
- Open parent task ‚Üí Check Subtasks column ‚Üí Open subtask ‚Üí Check status ‚Üí Review Subtasks Registry

### Graph Integrity

All edges are **bidirectional**. If you can go from A‚ÜíB, you can also go from B‚ÜíA:
- Task ‚Üî Log (via log#anchor references and task metadata in logs)
- Task ‚Üî File (via footnotes and embedded FlowSpace IDs)
- Task ‚Üî Footnote (via Notes column and footnote task references)
- Plan Task ‚Üî Dossier Task (via "Supports plan task X.Y" and task correlation)
- Parent Task ‚Üî Subtask (via Subtasks column and Parent Context section)

**Validation**: Run `/plan-7-code-review` to verify all bidirectional links are intact and synchronized.

```

Next step (when happy): Run **/plan-4-complete-the-plan** to validate readiness.
